//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
#[cfg(feature = "objc2")]
use objc2::__framework_prelude::*;
#[cfg(feature = "objc2-av-foundation")]
use objc2_av_foundation::*;
#[cfg(feature = "objc2-core-location")]
use objc2_core_location::*;
#[cfg(feature = "objc2-foundation")]
use objc2_foundation::*;

use crate::*;

/// Option set indicating semantic understanding types of the image frame.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arframesemantics?language=objc)
// NS_OPTIONS
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARFrameSemantics(pub NSUInteger);
#[cfg(feature = "objc2")]
bitflags::bitflags! {
    impl ARFrameSemantics: NSUInteger {
/// No semantic operation is run.
        #[doc(alias = "ARFrameSemanticNone")]
        const None = 0;
/// Person segmentation.
///
/// A pixel in the image frame that gets classified as person will have an intensity value equal to 'ARSegmentationClassPerson'.
///
/// See: -[ARFrame segmentationBuffer]
///
/// See: ARSegmentationClass
        #[doc(alias = "ARFrameSemanticPersonSegmentation")]
        const PersonSegmentation = 1<<0;
/// Person segmentation with depth.
///
/// A pixel in the image frame that gets classified as person will have an intensity value equal to 'ARSegmentationClassPerson'.
/// Additionally, every pixel in the image frame that gets classified as person will also have a depth value.
///
/// See: -[ARFrame estimatedDepthData]
///
/// See: -[ARFrame segmentationBuffer]
        #[doc(alias = "ARFrameSemanticPersonSegmentationWithDepth")]
        const PersonSegmentationWithDepth = (1<<1)|(1<<0);
/// Body detection.
///
/// Once activated an ARFrame will contain information about a detected body.
///
/// See: -[ARFrame detectedBody]
///
/// See: ARBody2D
        #[doc(alias = "ARFrameSemanticBodyDetection")]
        const BodyDetection = 1<<2;
/// Scene Depth.
///
/// Each capturedImage will have an associated scene depth data.
///
/// See: - [ARFrame sceneDepth]
        #[doc(alias = "ARFrameSemanticSceneDepth")]
        const SceneDepth = 1<<3;
/// Smoothed Scene Depth.
///
/// Each capturedImage will have an associated scene depth data that is temporally smoothed.
///
/// See: - [ARFrame smoothedSceneDepth]
        #[doc(alias = "ARFrameSemanticSmoothedSceneDepth")]
        const SmoothedSceneDepth = 1<<4;
    }
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARFrameSemantics {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARFrameSemantics {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Enum constants for indicating the world alignment.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arworldalignment?language=objc)
// NS_ENUM
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARWorldAlignment(pub NSInteger);
#[cfg(feature = "objc2")]
impl ARWorldAlignment {
    /// Aligns the world with gravity that is defined by vector (0, -1, 0).
    #[doc(alias = "ARWorldAlignmentGravity")]
    pub const Gravity: Self = Self(0);
    /// Aligns the world with gravity that is defined by the vector (0, -1, 0)
    /// and heading (w.r.t. True North) that is given by the vector (0, 0, -1).
    #[doc(alias = "ARWorldAlignmentGravityAndHeading")]
    pub const GravityAndHeading: Self = Self(1);
    /// Aligns the world with the cameraâ€™s orientation.
    #[doc(alias = "ARWorldAlignmentCamera")]
    pub const Camera: Self = Self(2);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARWorldAlignment {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARWorldAlignment {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Enum constants for indicating the mode of environment texturing to run.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arenvironmenttexturing?language=objc)
// NS_ENUM
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct AREnvironmentTexturing(pub NSInteger);
#[cfg(feature = "objc2")]
impl AREnvironmentTexturing {
    /// No texture information is gathered.
    #[doc(alias = "AREnvironmentTexturingNone")]
    pub const None: Self = Self(0);
    /// Texture information is gathered for the environment.
    /// Environment textures will be generated for AREnvironmentProbes added to the session.
    #[doc(alias = "AREnvironmentTexturingManual")]
    pub const Manual: Self = Self(1);
    /// Texture information is gathered for the environment and probes automatically placed in the scene.
    #[doc(alias = "AREnvironmentTexturingAutomatic")]
    pub const Automatic: Self = Self(2);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for AREnvironmentTexturing {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for AREnvironmentTexturing {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// Types of scene reconstruction.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arscenereconstruction?language=objc)
// NS_OPTIONS
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARSceneReconstruction(pub NSUInteger);
#[cfg(feature = "objc2")]
bitflags::bitflags! {
    impl ARSceneReconstruction: NSUInteger {
/// No scene reconstruction is run.
        #[doc(alias = "ARSceneReconstructionNone")]
        const None = 0;
/// Scene reconstruction generates a mesh of the world
        #[doc(alias = "ARSceneReconstructionMesh")]
        const Mesh = 1<<0;
/// Scene reconstruction generates a mesh of the world with classification for each face.
        #[doc(alias = "ARSceneReconstructionMeshWithClassification")]
        const MeshWithClassification = (1<<1)|(1<<0);
    }
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARSceneReconstruction {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARSceneReconstruction {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

#[cfg(feature = "objc2")]
extern_class!(
    /// An object to describe and configure the Augmented Reality techniques to be used in an ARSession.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arconfiguration?language=objc)
    #[unsafe(super(NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARConfiguration {
        /// Determines whether this device supports the ARConfiguration.
        #[method(isSupported)]
        pub unsafe fn isSupported() -> bool;

        #[cfg(all(feature = "ARVideoFormat", feature = "objc2-foundation"))]
        /// A list of supported video formats for this configuration and device.
        ///
        /// The first element in the list is the default format for session output.
        #[method_id(@__method_family Other supportedVideoFormats)]
        pub unsafe fn supportedVideoFormats() -> Retained<NSArray<ARVideoFormat>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Video format of the session output.
        #[method_id(@__method_family Other videoFormat)]
        pub unsafe fn videoFormat(&self) -> Retained<ARVideoFormat>;

        #[cfg(feature = "ARVideoFormat")]
        /// Setter for [`videoFormat`][Self::videoFormat].
        #[method(setVideoFormat:)]
        pub unsafe fn setVideoFormat(&self, video_format: &ARVideoFormat);

        /// Determines how the coordinate system should be aligned with the world.
        ///
        /// The default is ARWorldAlignmentGravity.
        #[method(worldAlignment)]
        pub unsafe fn worldAlignment(&self) -> ARWorldAlignment;

        /// Setter for [`worldAlignment`][Self::worldAlignment].
        #[method(setWorldAlignment:)]
        pub unsafe fn setWorldAlignment(&self, world_alignment: ARWorldAlignment);

        /// Enable or disable light estimation.
        ///
        /// Enabled by default.
        #[method(isLightEstimationEnabled)]
        pub unsafe fn isLightEstimationEnabled(&self) -> bool;

        /// Setter for [`isLightEstimationEnabled`][Self::isLightEstimationEnabled].
        #[method(setLightEstimationEnabled:)]
        pub unsafe fn setLightEstimationEnabled(&self, light_estimation_enabled: bool);

        /// Determines whether to capture and provide audio data.
        ///
        /// Disabled by default.
        #[method(providesAudioData)]
        pub unsafe fn providesAudioData(&self) -> bool;

        /// Setter for [`providesAudioData`][Self::providesAudioData].
        #[method(setProvidesAudioData:)]
        pub unsafe fn setProvidesAudioData(&self, provides_audio_data: bool);

        /// The type of semantic understanding to provide with each frame.
        ///
        ///
        /// Use the `supportsFrameSemantics` class method to check if the configuration type you intend to run supports the set of frame semantics. For example, when running a session with
        /// a configuration of type ARWorldTrackingConfiguration one would need to use `+[ ARWorldTrackingConfiguration supportsFrameSemantics:]` to perform said check.
        /// An exception is thrown if the option
        /// is not supported. Defaults to ARFrameSemanticNone.
        ///
        /// See: ARFrameSemantics
        ///
        /// See: +[ARConfiguration supportsFrameSemantics:]
        #[method(frameSemantics)]
        pub unsafe fn frameSemantics(&self) -> ARFrameSemantics;

        /// Setter for [`frameSemantics`][Self::frameSemantics].
        #[method(setFrameSemantics:)]
        pub unsafe fn setFrameSemantics(&self, frame_semantics: ARFrameSemantics);

        /// Determines whether the type of frame semantics is supported by the device and ARConfiguration class.
        ///
        ///
        /// Semantic frame understanding is not supported on all devices. Use the `supportsFrameSemantics` class method to check if the configuration type you intend to run supports the
        /// set of frame semantics. For example, when running a session with a configuration of type ARWorldTrackingConfiguration one would need to use
        /// `+[ ARWorldTrackingConfiguration supportsFrameSemantics:]` to perform said check.
        ///
        /// See: ARFrameSemantics
        #[method(supportsFrameSemantics:)]
        pub unsafe fn supportsFrameSemantics(frame_semantics: ARFrameSemantics) -> bool;

        #[cfg(feature = "objc2-av-foundation")]
        /// Returns a pointer to the capture device of the camera that's used for rendering, so developers can adjust capture settings.
        ///
        /// May return nil if it is not recommended to modify capture settings, for example if the primary camera is used for tracking.
        #[method_id(@__method_family Other configurableCaptureDeviceForPrimaryCamera)]
        pub unsafe fn configurableCaptureDeviceForPrimaryCamera(
        ) -> Option<Retained<AVCaptureDevice>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Returns a video format using a 4K resolution from the list of supported video formats.
        ///
        /// May return nil if 4K is not supported for this configuration or device.
        #[method_id(@__method_family Other recommendedVideoFormatFor4KResolution)]
        pub unsafe fn recommendedVideoFormatFor4KResolution() -> Option<Retained<ARVideoFormat>>;

        #[cfg(feature = "ARVideoFormat")]
        /// Returns a recommended video format that supports capturing high resolution frames with a significantly higher resolution than the streaming camera resolution.
        ///
        /// Using this format may consume more power. Other video formats may support capturing high resolution frames as well, albeit at a lower quality or resolution.
        ///
        /// See: [ARSession captureHighResolutionFrameWithCompletion:]
        #[method_id(@__method_family Other recommendedVideoFormatForHighResolutionFrameCapturing)]
        pub unsafe fn recommendedVideoFormatForHighResolutionFrameCapturing(
        ) -> Option<Retained<ARVideoFormat>>;

        /// Whether HDR capturing is allowed if the current video format supports it. Defaults to
        /// `NO.`
        #[method(videoHDRAllowed)]
        pub unsafe fn videoHDRAllowed(&self) -> bool;

        /// Setter for [`videoHDRAllowed`][Self::videoHDRAllowed].
        #[method(setVideoHDRAllowed:)]
        pub unsafe fn setVideoHDRAllowed(&self, video_hdr_allowed: bool);

        /// Unavailable
        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running world tracking.
    ///
    ///
    /// World tracking provides 6 degrees of freedom tracking of the device.
    /// By finding feature points in the scene, world tracking enables performing hit-tests against the frame.
    /// Tracking can no longer be resumed once the session is paused.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arworldtrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARWorldTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARWorldTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARWorldTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARWorldTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARWorldTrackingConfiguration {
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[method(isAutoFocusEnabled)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[method(setAutoFocusEnabled:)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[method(environmentTexturing)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[method(setEnvironmentTexturing:)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[method(wantsHDREnvironmentTextures)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[method(setWantsHDREnvironmentTextures:)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[method(planeDetection)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[method(setPlaneDetection:)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[method_id(@__method_family Other initialWorldMap)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[method(setInitialWorldMap:)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[method_id(@__method_family Other detectionImages)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[method(setDetectionImages:)]
        pub unsafe fn setDetectionImages(&self, detection_images: Option<&NSSet<ARReferenceImage>>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[method(automaticImageScaleEstimationEnabled)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[method(setAutomaticImageScaleEstimationEnabled:)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[method(maximumNumberOfTrackedImages)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[method(setMaximumNumberOfTrackedImages:)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Objects to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified objects. When an object is detected an ARObjectAnchor will be added to the session.
        #[method_id(@__method_family Other detectionObjects)]
        pub unsafe fn detectionObjects(&self) -> Retained<NSSet<ARReferenceObject>>;

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Setter for [`detectionObjects`][Self::detectionObjects].
        #[method(setDetectionObjects:)]
        pub unsafe fn setDetectionObjects(&self, detection_objects: &NSSet<ARReferenceObject>);

        /// Enable/disable a collaborative session. Disabled by default.
        ///
        ///
        /// When enabled, ARSession will output collaboration data for other participants using its delegate didOutputCollaborationData.
        /// It is the responsibility of the caller to send the data to each participant. When data is received by a participant, it
        /// should be passed to the ARSession by calling updateWithCollaborationData.
        #[method(isCollaborationEnabled)]
        pub unsafe fn isCollaborationEnabled(&self) -> bool;

        /// Setter for [`isCollaborationEnabled`][Self::isCollaborationEnabled].
        #[method(setCollaborationEnabled:)]
        pub unsafe fn setCollaborationEnabled(&self, collaboration_enabled: bool);

        /// Indicates whether user face tracking using the front facing camera can be enabled on this device.
        #[method(supportsUserFaceTracking)]
        pub unsafe fn supportsUserFaceTracking() -> bool;

        /// Enable or disable running Face Tracking using the front facing camera. Disabled by default.
        /// When enabled, ARSession detects faces (if visible in the front-facing camera image) and adds to its list of anchors,
        /// an ARFaceAnchor object representing each face.
        ///
        ///
        /// The transform of the ARFaceAnchor objects will be in the world coordinate space.
        ///
        /// See: ARFaceAnchor
        #[method(userFaceTrackingEnabled)]
        pub unsafe fn userFaceTrackingEnabled(&self) -> bool;

        /// Setter for [`userFaceTrackingEnabled`][Self::userFaceTrackingEnabled].
        #[method(setUserFaceTrackingEnabled:)]
        pub unsafe fn setUserFaceTrackingEnabled(&self, user_face_tracking_enabled: bool);

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[method(appClipCodeTrackingEnabled)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[method(setAppClipCodeTrackingEnabled:)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[method(supportsAppClipCodeTracking)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        /// Indicates whether the scene reconstruction type is supported for the configuration on this device.
        #[method(supportsSceneReconstruction:)]
        pub unsafe fn supportsSceneReconstruction(
            scene_reconstruction: ARSceneReconstruction,
        ) -> bool;

        /// Type of scene reconstruction to run. Defaults to ARSceneReconstructionNone.
        ///
        /// See: ARMeshAnchor
        ///
        /// If set to a value other than ARSceneReconstructionNone, output of scene reconstruction will be added to the session as
        /// ARMeshAnchor objects.
        #[method(sceneReconstruction)]
        pub unsafe fn sceneReconstruction(&self) -> ARSceneReconstruction;

        /// Setter for [`sceneReconstruction`][Self::sceneReconstruction].
        #[method(setSceneReconstruction:)]
        pub unsafe fn setSceneReconstruction(&self, scene_reconstruction: ARSceneReconstruction);

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running orientation tracking.
    ///
    ///
    /// Orientation tracking provides 3 degrees of freedom tracking of the device.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arorientationtrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct AROrientationTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for AROrientationTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for AROrientationTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for AROrientationTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl AROrientationTrackingConfiguration {
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[method(isAutoFocusEnabled)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[method(setAutoFocusEnabled:)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running face tracking.
    ///
    ///
    /// Face tracking uses the front facing camera to track the face in 3D providing details on the topology and expression of the face.
    /// A detected face will be added to the session as an ARFaceAnchor object which contains information about head pose, mesh, eye pose, and blend shape
    /// coefficients. If light estimation is enabled the detected face will be treated as a light probe and used to estimate the direction of incoming light.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arfacetrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARFaceTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARFaceTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARFaceTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARFaceTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARFaceTrackingConfiguration {
        /// Maximum number of faces which can be tracked simultaneously.
        #[method(supportedNumberOfTrackedFaces)]
        pub unsafe fn supportedNumberOfTrackedFaces() -> NSInteger;

        /// Maximum number of faces to track simultaneously.
        ///
        /// Setting the maximum number of tracked faces will limit the number of faces that can be tracked in a given frame.
        /// If more than the maximum is visible, only the faces already being tracked will continue to track until tracking is lost or another face is removed.
        /// Default value is one.
        #[method(maximumNumberOfTrackedFaces)]
        pub unsafe fn maximumNumberOfTrackedFaces(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedFaces`][Self::maximumNumberOfTrackedFaces].
        #[method(setMaximumNumberOfTrackedFaces:)]
        pub unsafe fn setMaximumNumberOfTrackedFaces(
            &self,
            maximum_number_of_tracked_faces: NSInteger,
        );

        /// Indicates whether world tracking can be enabled on this device.
        #[method(supportsWorldTracking)]
        pub unsafe fn supportsWorldTracking() -> bool;

        /// Enable or disable World Tracking. Disabled by default.
        ///
        ///
        /// When enabled, ARSession uses the back facing camera to track the device's orientation and position in the world. The camera transform and the ARFaceAnchor transform will be in the world coordinate space.
        #[method(isWorldTrackingEnabled)]
        pub unsafe fn isWorldTrackingEnabled(&self) -> bool;

        /// Setter for [`isWorldTrackingEnabled`][Self::isWorldTrackingEnabled].
        #[method(setWorldTrackingEnabled:)]
        pub unsafe fn setWorldTrackingEnabled(&self, world_tracking_enabled: bool);

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running image tracking.
    ///
    ///
    /// Image tracking provides 6 degrees of freedom tracking of known images. Four images may be tracked simultaneously.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arimagetrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARImageTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARImageTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARImageTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARImageTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARImageTrackingConfiguration {
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[method(isAutoFocusEnabled)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[method(setAutoFocusEnabled:)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to track in the scene.
        #[method_id(@__method_family Other trackingImages)]
        pub unsafe fn trackingImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`trackingImages`][Self::trackingImages].
        #[method(setTrackingImages:)]
        pub unsafe fn setTrackingImages(&self, tracking_images: &NSSet<ARReferenceImage>);

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Default value is one.
        #[method(maximumNumberOfTrackedImages)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[method(setMaximumNumberOfTrackedImages:)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for scanning objects.
    ///
    ///
    /// The object scanning configuration runs world tracking, capturing additional detail in order to create reference objects.
    /// Running object scanning will consume additional power in order to provide more detailed features.
    /// The createReferenceObject method can be called on the session to capture a scan of an object in the world.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arobjectscanningconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARObjectScanningConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARObjectScanningConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARObjectScanningConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARObjectScanningConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARObjectScanningConfiguration {
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[method(isAutoFocusEnabled)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[method(setAutoFocusEnabled:)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[method(planeDetection)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[method(setPlaneDetection:)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running body tracking.
    ///
    ///
    /// Body tracking provides 6 degrees of freedom tracking of a detected body in the scene. By default, ARFrameSemanticBodyDetection will be
    /// enabled.
    ///
    /// See: ARBodyAnchor
    ///
    /// See: -[ARFrame detectedBody]
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arbodytrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARBodyTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARBodyTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARBodyTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARBodyTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARBodyTrackingConfiguration {
        /// Enable or disable continuous auto focus.
        ///
        /// Enabled by default.
        #[method(isAutoFocusEnabled)]
        pub unsafe fn isAutoFocusEnabled(&self) -> bool;

        /// Setter for [`isAutoFocusEnabled`][Self::isAutoFocusEnabled].
        #[method(setAutoFocusEnabled:)]
        pub unsafe fn setAutoFocusEnabled(&self, auto_focus_enabled: bool);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[method_id(@__method_family Other initialWorldMap)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[method(setInitialWorldMap:)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[method(environmentTexturing)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[method(setEnvironmentTexturing:)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[method(wantsHDREnvironmentTextures)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[method(setWantsHDREnvironmentTextures:)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[method(planeDetection)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[method(setPlaneDetection:)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[method_id(@__method_family Other detectionImages)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[method(setDetectionImages:)]
        pub unsafe fn setDetectionImages(&self, detection_images: &NSSet<ARReferenceImage>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[method(automaticImageScaleEstimationEnabled)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[method(setAutomaticImageScaleEstimationEnabled:)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Enables the estimation of a scale factor which may be used to correct the physical size of a skeleton in 3D.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the default one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARBodyAnchor.
        ///
        /// Note: When set to true the transform of a returned ARBodyAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[method(automaticSkeletonScaleEstimationEnabled)]
        pub unsafe fn automaticSkeletonScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticSkeletonScaleEstimationEnabled`][Self::automaticSkeletonScaleEstimationEnabled].
        #[method(setAutomaticSkeletonScaleEstimationEnabled:)]
        pub unsafe fn setAutomaticSkeletonScaleEstimationEnabled(
            &self,
            automatic_skeleton_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[method(maximumNumberOfTrackedImages)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[method(setMaximumNumberOfTrackedImages:)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[method(appClipCodeTrackingEnabled)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[method(setAppClipCodeTrackingEnabled:)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[method(supportsAppClipCodeTracking)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running positional tracking.
    ///
    ///
    /// Positional tracking provides 6 degrees of freedom tracking of the device by running the camera at lowest possible resolution and frame rate.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arpositionaltrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARPositionalTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARPositionalTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARPositionalTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARPositionalTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARPositionalTrackingConfiguration {
        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[method(planeDetection)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[method(setPlaneDetection:)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(feature = "ARWorldMap")]
        /// The initial map of the physical space that world tracking will localize to and track.
        ///
        /// If set, the session will attempt to localize to the provided map with
        /// a limited tracking state until localization is successful or run is called again
        /// with a different (or no) initial map specified. Once localized, the map will be extended
        /// and can again be saved using the `getCurrentWorldMap` method on the session.
        #[method_id(@__method_family Other initialWorldMap)]
        pub unsafe fn initialWorldMap(&self) -> Option<Retained<ARWorldMap>>;

        #[cfg(feature = "ARWorldMap")]
        /// Setter for [`initialWorldMap`][Self::initialWorldMap].
        #[method(setInitialWorldMap:)]
        pub unsafe fn setInitialWorldMap(&self, initial_world_map: Option<&ARWorldMap>);

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

#[cfg(feature = "objc2")]
extern_class!(
    /// A configuration for running geographical world tracking.
    ///
    ///
    /// It allows placing geo-referenced anchors (ARGeoAnchor) in the scene by running world tracking with location and compass.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/argeotrackingconfiguration?language=objc)
    #[unsafe(super(ARConfiguration, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARGeoTrackingConfiguration;
);

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARGeoTrackingConfiguration {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARGeoTrackingConfiguration {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARGeoTrackingConfiguration {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARGeoTrackingConfiguration {
        /// Unavailable
        #[method(worldAlignment)]
        pub unsafe fn worldAlignment(&self) -> ARWorldAlignment;

        /// Setter for [`worldAlignment`][Self::worldAlignment].
        #[method(setWorldAlignment:)]
        pub unsafe fn setWorldAlignment(&self, world_alignment: ARWorldAlignment);

        /// The mode of environment texturing to run.
        ///
        /// If set, texture information will be accumulated and updated. Adding an AREnvironmentProbeAnchor to the session
        /// will get the current environment texture available from that probe's perspective which can be used for lighting
        /// virtual objects in the scene. Defaults to AREnvironmentTexturingNone.
        #[method(environmentTexturing)]
        pub unsafe fn environmentTexturing(&self) -> AREnvironmentTexturing;

        /// Setter for [`environmentTexturing`][Self::environmentTexturing].
        #[method(setEnvironmentTexturing:)]
        pub unsafe fn setEnvironmentTexturing(&self, environment_texturing: AREnvironmentTexturing);

        /// Determines whether environment textures will be provided with high dynamic range. Enabled by default.
        #[method(wantsHDREnvironmentTextures)]
        pub unsafe fn wantsHDREnvironmentTextures(&self) -> bool;

        /// Setter for [`wantsHDREnvironmentTextures`][Self::wantsHDREnvironmentTextures].
        #[method(setWantsHDREnvironmentTextures:)]
        pub unsafe fn setWantsHDREnvironmentTextures(&self, wants_hdr_environment_textures: bool);

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Type of planes to detect in the scene.
        ///
        /// If set, new planes will continue to be detected and updated over time. Detected planes will be added to the session as
        /// ARPlaneAnchor objects. In the event that two planes are merged, the newer plane will be removed. Defaults to ARPlaneDetectionNone.
        #[method(planeDetection)]
        pub unsafe fn planeDetection(&self) -> ARPlaneDetection;

        #[cfg(feature = "ARPlaneDetectionTypes")]
        /// Setter for [`planeDetection`][Self::planeDetection].
        #[method(setPlaneDetection:)]
        pub unsafe fn setPlaneDetection(&self, plane_detection: ARPlaneDetection);

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Images to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified images. When an image is detected an ARImageAnchor will be added to the session.
        #[method_id(@__method_family Other detectionImages)]
        pub unsafe fn detectionImages(&self) -> Retained<NSSet<ARReferenceImage>>;

        #[cfg(all(feature = "ARReferenceImage", feature = "objc2-foundation"))]
        /// Setter for [`detectionImages`][Self::detectionImages].
        #[method(setDetectionImages:)]
        pub unsafe fn setDetectionImages(&self, detection_images: Option<&NSSet<ARReferenceImage>>);

        /// Enables the estimation of a scale factor which may be used to correct the physical size of an image.
        ///
        /// If set to true ARKit will attempt to use the computed camera positions in order to compute the scale by which the given physical size
        /// differs from the estimated one. The information about the estimated scale can be found as the property estimatedScaleFactor on the ARImageAnchor.
        ///
        /// Note: When set to true the transform of a returned ARImageAnchor will use the estimated scale factor to correct the translation. Default value is NO.
        #[method(automaticImageScaleEstimationEnabled)]
        pub unsafe fn automaticImageScaleEstimationEnabled(&self) -> bool;

        /// Setter for [`automaticImageScaleEstimationEnabled`][Self::automaticImageScaleEstimationEnabled].
        #[method(setAutomaticImageScaleEstimationEnabled:)]
        pub unsafe fn setAutomaticImageScaleEstimationEnabled(
            &self,
            automatic_image_scale_estimation_enabled: bool,
        );

        /// Maximum number of images to track simultaneously.
        ///
        /// Setting the maximum number of tracked images will limit the number of images that can be tracked in a given frame.
        /// If more than the maximum is visible, only the images already being tracked will continue to track until tracking is lost or another image is removed.
        /// Images will continue to be detected regardless of images tracked. Default value is zero.
        #[method(maximumNumberOfTrackedImages)]
        pub unsafe fn maximumNumberOfTrackedImages(&self) -> NSInteger;

        /// Setter for [`maximumNumberOfTrackedImages`][Self::maximumNumberOfTrackedImages].
        #[method(setMaximumNumberOfTrackedImages:)]
        pub unsafe fn setMaximumNumberOfTrackedImages(
            &self,
            maximum_number_of_tracked_images: NSInteger,
        );

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Objects to detect in the scene.
        ///
        /// If set the session will attempt to detect the specified objects. When an object is detected an ARObjectAnchor will be added to the session.
        #[method_id(@__method_family Other detectionObjects)]
        pub unsafe fn detectionObjects(&self) -> Retained<NSSet<ARReferenceObject>>;

        #[cfg(all(feature = "ARReferenceObject", feature = "objc2-foundation"))]
        /// Setter for [`detectionObjects`][Self::detectionObjects].
        #[method(setDetectionObjects:)]
        pub unsafe fn setDetectionObjects(&self, detection_objects: &NSSet<ARReferenceObject>);

        /// Enable or disable app clip code tracking. Disabled by default. When enabled, detected app clip codes will be surfaced as an ARAppClipCodeAnchor.
        #[method(appClipCodeTrackingEnabled)]
        pub unsafe fn appClipCodeTrackingEnabled(&self) -> bool;

        /// Setter for [`appClipCodeTrackingEnabled`][Self::appClipCodeTrackingEnabled].
        #[method(setAppClipCodeTrackingEnabled:)]
        pub unsafe fn setAppClipCodeTrackingEnabled(&self, app_clip_code_tracking_enabled: bool);

        /// Indicates whether app clip code tracking can be enabled on this device.
        #[method(supportsAppClipCodeTracking)]
        pub unsafe fn supportsAppClipCodeTracking() -> bool;

        #[cfg(all(feature = "block2", feature = "objc2-foundation"))]
        /// Determines the availability of geo tracking at the current location.
        ///
        ///
        /// This method will attempt to acquire a location fix on a background thread, then check availability.
        ///
        ///
        /// Parameter `completionHandler`: Completion handler that is called when availability has been determined. This handler is executed on an arbitrary serial queue. It takes the following parameters:
        /// isAvailable - True if geo tracking is available at the current location, otherwise false.
        /// error - An error that indicates why geo tracking is not available at the current location.
        #[method(checkAvailabilityWithCompletionHandler:)]
        pub unsafe fn checkAvailabilityWithCompletionHandler(
            completion_handler: &block2::Block<dyn Fn(Bool, *mut NSError)>,
        );

        #[cfg(all(
            feature = "block2",
            feature = "objc2-core-location",
            feature = "objc2-foundation"
        ))]
        /// Determines the availability of geo tracking at the given location.
        ///
        /// Parameter `coordinate`: Location at which to check.
        ///
        /// Parameter `completionHandler`: Completion handler that is called when availability has been determined. This handler is executed on an arbitrary serial queue. It takes the following parameters:
        /// isAvailable - True if geo tracking is available at the given location, otherwise false.
        /// error - An error that indicates why geo tracking is not available at the given location.
        #[method(checkAvailabilityAtCoordinate:completionHandler:)]
        pub unsafe fn checkAvailabilityAtCoordinate_completionHandler(
            coordinate: CLLocationCoordinate2D,
            completion_handler: &block2::Block<dyn Fn(Bool, *mut NSError)>,
        );

        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);
