//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
#[cfg(feature = "objc2")]
use objc2::__framework_prelude::*;
#[cfg(feature = "objc2-av-foundation")]
use objc2_av_foundation::*;
#[cfg(feature = "objc2-core-foundation")]
use objc2_core_foundation::*;
#[cfg(feature = "objc2-core-video")]
use objc2_core_video::*;
#[cfg(feature = "objc2-foundation")]
use objc2_foundation::*;
#[cfg(feature = "objc2-metal")]
use objc2_metal::*;
#[cfg(feature = "objc2-ui-kit")]
use objc2_ui_kit::*;

use crate::*;

/// Segmentation classes which defines a pixel's semantic label.
///
/// When running a configuration with 'ARFrameSemanticPersonSegmentation' every pixel in the
/// segmentationBuffer on the ARFrame will conform to one of these classes.
///
/// See: -[ARConfiguration setFrameSemantics:]
///
/// See: -[ARFrame segmentationBuffer]
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arsegmentationclass?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARSegmentationClass(pub u8);
impl ARSegmentationClass {
    #[doc(alias = "ARSegmentationClassNone")]
    pub const None: Self = Self(0);
    #[doc(alias = "ARSegmentationClassPerson")]
    pub const Person: Self = Self(255);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARSegmentationClass {
    const ENCODING: Encoding = u8::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARSegmentationClass {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

/// A value describing the world mapping status for the area visible in a given frame.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arworldmappingstatus?language=objc)
// NS_ENUM
#[cfg(feature = "objc2")]
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ARWorldMappingStatus(pub NSInteger);
#[cfg(feature = "objc2")]
impl ARWorldMappingStatus {
    /// World mapping is not available.
    #[doc(alias = "ARWorldMappingStatusNotAvailable")]
    pub const NotAvailable: Self = Self(0);
    /// World mapping is available but has limited features.
    /// For the device's current position, the session’s world map is not recommended for relocalization.
    #[doc(alias = "ARWorldMappingStatusLimited")]
    pub const Limited: Self = Self(1);
    /// World mapping is actively extending the map with the user's motion.
    /// The world map will be relocalizable for previously visited areas but is still being updated for the current space.
    #[doc(alias = "ARWorldMappingStatusExtending")]
    pub const Extending: Self = Self(2);
    /// World mapping has adequately mapped the visible area.
    /// The map can be used to relocalize for the device's current position.
    #[doc(alias = "ARWorldMappingStatusMapped")]
    pub const Mapped: Self = Self(3);
}

#[cfg(feature = "objc2")]
unsafe impl Encode for ARWorldMappingStatus {
    const ENCODING: Encoding = NSInteger::ENCODING;
}

#[cfg(feature = "objc2")]
unsafe impl RefEncode for ARWorldMappingStatus {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

#[cfg(feature = "objc2")]
extern_class!(
    /// An object encapsulating the state of everything being tracked for a given moment in time.
    ///
    /// The model provides a snapshot of all data needed to render a given frame.
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/arkit/arframe?language=objc)
    #[unsafe(super(NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "objc2")]
    pub struct ARFrame;
);

#[cfg(feature = "objc2")]
unsafe impl Send for ARFrame {}

#[cfg(feature = "objc2")]
unsafe impl Sync for ARFrame {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl NSCopying for ARFrame {}

#[cfg(all(feature = "objc2", feature = "objc2-foundation"))]
unsafe impl CopyingHelper for ARFrame {
    type Result = Self;
}

#[cfg(feature = "objc2")]
unsafe impl NSObjectProtocol for ARFrame {}

#[cfg(feature = "objc2")]
extern_methods!(
    #[cfg(feature = "objc2")]
    unsafe impl ARFrame {
        #[cfg(feature = "objc2-foundation")]
        /// A timestamp identifying the frame.
        #[method(timestamp)]
        pub unsafe fn timestamp(&self) -> NSTimeInterval;

        #[cfg(feature = "objc2-core-video")]
        /// The frame’s captured image.
        #[method_id(@__method_family Other capturedImage)]
        pub unsafe fn capturedImage(&self) -> Retained<CVPixelBuffer>;

        #[cfg(feature = "objc2-foundation")]
        /// A dictionary of EXIF metadata for the captured image.
        #[method_id(@__method_family Other exifData)]
        pub unsafe fn exifData(&self) -> Retained<NSDictionary<NSString, AnyObject>>;

        #[cfg(feature = "objc2-metal")]
        /// A tileable texture that contains image noise matching the current camera streams
        /// noise properties.
        ///
        ///
        /// A camera stream depicts image noise that gives the captured image
        /// a grainy look and varies with light conditions.
        /// The variations are stored along the depth dimension of the camera grain texture
        /// and can be selected at runtime using the camera grain intensity of the current frame.
        #[method_id(@__method_family Other cameraGrainTexture)]
        pub unsafe fn cameraGrainTexture(&self)
            -> Option<Retained<ProtocolObject<dyn MTLTexture>>>;

        /// The frame’s camera grain intensity in range 0 to 1.
        ///
        ///
        /// A camera stream depicts image noise that gives the captured image
        /// a grainy look and varies with light conditions.
        /// The camera grain intensity can be used to select a texture slice from the frames
        /// camera grain texture.
        #[method(cameraGrainIntensity)]
        pub unsafe fn cameraGrainIntensity(&self) -> c_float;

        #[cfg(feature = "objc2-av-foundation")]
        /// The frame’s captured depth data.
        ///
        /// Depth data is only provided with face tracking on frames where depth data was captured.
        #[method_id(@__method_family Other capturedDepthData)]
        pub unsafe fn capturedDepthData(&self) -> Option<Retained<AVDepthData>>;

        #[cfg(feature = "objc2-foundation")]
        /// A timestamp identifying the depth data.
        #[method(capturedDepthDataTimestamp)]
        pub unsafe fn capturedDepthDataTimestamp(&self) -> NSTimeInterval;

        #[cfg(feature = "ARCamera")]
        /// The camera used to capture the frame’s image.
        ///
        /// The camera provides the device’s position and orientation as well as camera parameters.
        #[method_id(@__method_family Other camera)]
        pub unsafe fn camera(&self) -> Retained<ARCamera>;

        #[cfg(all(feature = "ARAnchor", feature = "objc2-foundation"))]
        /// A list of anchors in the scene.
        #[method_id(@__method_family Other anchors)]
        pub unsafe fn anchors(&self) -> Retained<NSArray<ARAnchor>>;

        #[cfg(feature = "ARLightEstimate")]
        /// A light estimate representing the light in the scene.
        ///
        /// Returns nil if there is no light estimation.
        #[method_id(@__method_family Other lightEstimate)]
        pub unsafe fn lightEstimate(&self) -> Option<Retained<ARLightEstimate>>;

        #[cfg(feature = "ARPointCloud")]
        /// Feature points in the scene with respect to the frame’s origin.
        ///
        /// The feature points are only provided for configurations using world tracking.
        #[method_id(@__method_family Other rawFeaturePoints)]
        pub unsafe fn rawFeaturePoints(&self) -> Option<Retained<ARPointCloud>>;

        /// The status of world mapping for the area visible to the frame.
        ///
        /// This can be used to identify the state of the world map for the visible area and if additional scanning
        /// should be done before saving a world map.
        #[method(worldMappingStatus)]
        pub unsafe fn worldMappingStatus(&self) -> ARWorldMappingStatus;

        #[cfg(feature = "objc2-core-video")]
        /// A buffer that represents the segmented content of the capturedImage.
        ///
        /// In order to identify to which class a pixel has been classified one needs to compare its intensity value with the values
        /// found in `ARSegmentationClass`.
        ///
        /// See: ARSegmentationClass
        ///
        /// See: -[ARConfiguration setFrameSemantics:]
        #[method_id(@__method_family Other segmentationBuffer)]
        pub unsafe fn segmentationBuffer(&self) -> Option<Retained<CVPixelBuffer>>;

        #[cfg(feature = "objc2-core-video")]
        /// A buffer that represents the estimated depth values for a performed segmentation.
        ///
        /// For each non-background pixel in the segmentation buffer the corresponding depth value can be accessed in this buffer.
        ///
        /// See: -[ARConfiguration setFrameSemantics:]
        ///
        /// See: -[ARFrame segmentationBuffer]
        #[method_id(@__method_family Other estimatedDepthData)]
        pub unsafe fn estimatedDepthData(&self) -> Option<Retained<CVPixelBuffer>>;

        #[cfg(feature = "ARBody2D")]
        /// A detected body in the current frame.
        ///
        /// See: -[ARConfiguration setFrameSemantics:]
        #[method_id(@__method_family Other detectedBody)]
        pub unsafe fn detectedBody(&self) -> Option<Retained<ARBody2D>>;

        #[cfg(feature = "ARGeoTrackingTypes")]
        /// The status of geo tracking.
        #[method_id(@__method_family Other geoTrackingStatus)]
        pub unsafe fn geoTrackingStatus(&self) -> Option<Retained<ARGeoTrackingStatus>>;

        #[cfg(feature = "ARDepthData")]
        /// Scene depth data.
        ///
        /// See: ARFrameSemanticSceneDepth.
        ///
        /// See: -[ARConfiguration setFrameSemantics:]
        #[method_id(@__method_family Other sceneDepth)]
        pub unsafe fn sceneDepth(&self) -> Option<Retained<ARDepthData>>;

        #[cfg(feature = "ARDepthData")]
        /// Scene depth data, smoothed for temporal consistency.
        ///
        /// See: ARFrameSemanticSmoothedSceneDepth.
        ///
        /// See: -[ARConfiguration setFrameSemantics:]
        #[method_id(@__method_family Other smoothedSceneDepth)]
        pub unsafe fn smoothedSceneDepth(&self) -> Option<Retained<ARDepthData>>;

        #[cfg(all(
            feature = "ARHitTestResult",
            feature = "objc2-core-foundation",
            feature = "objc2-foundation"
        ))]
        /// Searches the frame for objects corresponding to a point in the captured image.
        ///
        ///
        /// A 2D point in the captured image’s coordinate space can refer to any point along a line segment
        /// in the 3D coordinate space. Hit-testing is the process of finding objects in the world located along this line segment.
        ///
        /// Parameter `point`: A point in the image-space coordinate system of the captured image.
        /// Values should range from (0,0) - upper left corner to (1,1) - lower right corner.
        ///
        /// Parameter `types`: The types of results to search for.
        ///
        /// Returns: An array of all hit-test results sorted from nearest to farthest.
        #[deprecated = "Use [ARSession raycast:]"]
        #[method_id(@__method_family Other hitTest:types:)]
        pub unsafe fn hitTest_types(
            &self,
            point: CGPoint,
            types: ARHitTestResultType,
        ) -> Retained<NSArray<ARHitTestResult>>;

        #[cfg(all(feature = "ARRaycastQuery", feature = "objc2-core-foundation"))]
        /// Creates a raycast query originating from the point on the captured image, aligned along the center of the field of view of the camera.
        ///
        /// A 2D point in the captured image’s coordinate space and the field of view of the frame's camera is used to create a ray in the 3D cooridnate space originating at the point.
        ///
        /// Parameter `point`: A point in the image-space coordinate system of the captured image.
        /// Values should range from (0,0) - upper left corner to (1,1) - lower right corner.
        ///
        /// Parameter `target`: Type of target where the ray should terminate.
        ///
        /// Parameter `alignment`: Alignment of the target.
        #[method_id(@__method_family Other raycastQueryFromPoint:allowingTarget:alignment:)]
        pub unsafe fn raycastQueryFromPoint_allowingTarget_alignment(
            &self,
            point: CGPoint,
            target: ARRaycastTarget,
            alignment: ARRaycastTargetAlignment,
        ) -> Retained<ARRaycastQuery>;

        #[cfg(all(feature = "objc2-core-foundation", feature = "objc2-ui-kit"))]
        /// Returns a display transform for the provided viewport size and orientation.
        ///
        ///
        /// The display transform can be used to convert normalized points in the image-space coordinate system
        /// of the captured image to normalized points in the view’s coordinate space. The transform provides the correct rotation
        /// and aspect-fill for presenting the captured image in the given orientation and size.
        ///
        /// Parameter `orientation`: The orientation of the viewport.
        ///
        /// Parameter `viewportSize`: The size of the viewport.
        #[method(displayTransformForOrientation:viewportSize:)]
        pub unsafe fn displayTransformForOrientation_viewportSize(
            &self,
            orientation: UIInterfaceOrientation,
            viewport_size: CGSize,
        ) -> CGAffineTransform;

        /// Unavailable
        #[method_id(@__method_family Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__method_family New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);
