//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ffi::*;
use core::ptr::NonNull;
use objc2::__framework_prelude::*;
use objc2_foundation::*;

use crate::*;

/// The activation modes for RNN operations.
///
/// See also [Apple's documentation](https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphrnnactivation?language=objc)
// NS_ENUM
#[repr(transparent)]
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct MPSGraphRNNActivation(pub NSUInteger);
impl MPSGraphRNNActivation {
    /// Defines a pass through activation.
    #[doc(alias = "MPSGraphRNNActivationNone")]
    pub const None: Self = Self(0);
    /// Defines a ReLU activation.
    #[doc(alias = "MPSGraphRNNActivationRelu")]
    pub const Relu: Self = Self(1);
    /// Defines a Tanh activation.
    #[doc(alias = "MPSGraphRNNActivationTanh")]
    pub const Tanh: Self = Self(2);
    /// Defines a Sigmoid activation.
    #[doc(alias = "MPSGraphRNNActivationSigmoid")]
    pub const Sigmoid: Self = Self(3);
    /// Defines a Hard sigmoid activation.
    #[doc(alias = "MPSGraphRNNActivationHardSigmoid")]
    pub const HardSigmoid: Self = Self(4);
}

unsafe impl Encode for MPSGraphRNNActivation {
    const ENCODING: Encoding = NSUInteger::ENCODING;
}

unsafe impl RefEncode for MPSGraphRNNActivation {
    const ENCODING_REF: Encoding = Encoding::Pointer(&Self::ENCODING);
}

extern_class!(
    /// The class that defines the parameters for a single gate RNN operation.
    ///
    /// Use this descriptor with the following ``MPSGraph`` methods:
    /// - ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:initState:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:initState:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:inputWeight:bias:initState:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:inputWeight:bias:initState:mask:descriptor:name:``
    /// - ``MPSGraph/singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:stateGradient:inputWeight:bias:initState:mask:descriptor:name:``
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphsinglegaternndescriptor?language=objc)
    #[unsafe(super(MPSGraphObject, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "MPSGraphCore")]
    pub struct MPSGraphSingleGateRNNDescriptor;
);

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSCopying for MPSGraphSingleGateRNNDescriptor {}

#[cfg(feature = "MPSGraphCore")]
unsafe impl CopyingHelper for MPSGraphSingleGateRNNDescriptor {
    type Result = Self;
}

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSObjectProtocol for MPSGraphSingleGateRNNDescriptor {}

extern_methods!(
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphSingleGateRNNDescriptor {
        /// A parameter that defines time direction of the input sequence.
        ///
        /// If set to `YES` then the input sequence is passed in reverse time order to the layer.
        /// Note: Ignored when `bidirectional = YES`.
        /// Default value: `NO`.
        #[method(reverse)]
        pub unsafe fn reverse(&self) -> bool;

        /// Setter for [`reverse`][Self::reverse].
        #[method(setReverse:)]
        pub unsafe fn setReverse(&self, reverse: bool);

        /// A parameter that defines a bidirectional RNN layer.
        ///
        /// If set to `YES` then the input sequence is traversed in both directions and the two results
        /// are concatenated together on the channel-axis.
        /// Default value: `NO`.
        #[method(bidirectional)]
        pub unsafe fn bidirectional(&self) -> bool;

        /// Setter for [`bidirectional`][Self::bidirectional].
        #[method(setBidirectional:)]
        pub unsafe fn setBidirectional(&self, bidirectional: bool);

        /// A parameter that makes the RNN layer support training.
        ///
        /// If set to `YES` then the layer will produce training state tensor as a secondary output.
        /// Default value: `NO`.
        #[method(training)]
        pub unsafe fn training(&self) -> bool;

        /// Setter for [`training`][Self::training].
        #[method(setTraining:)]
        pub unsafe fn setTraining(&self, training: bool);

        /// A parameter that defines the activation function to use with the RNN operation.
        ///
        /// Default value: `MPSGraphRNNActivationRelu`.
        #[method(activation)]
        pub unsafe fn activation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`activation`][Self::activation].
        #[method(setActivation:)]
        pub unsafe fn setActivation(&self, activation: MPSGraphRNNActivation);

        /// Creates a single gate RNN descriptor with default values.
        #[method_id(@__retain_semantics Other descriptor)]
        pub unsafe fn descriptor() -> Option<Retained<Self>>;
    }
);

extern_methods!(
    /// Methods declared on superclass `NSObject`
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphSingleGateRNNDescriptor {
        #[method_id(@__retain_semantics Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__retain_semantics New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

extern_class!(
    /// The class that defines the parameters for a long short-term memory (LSTM) operation.
    ///
    /// Use this descriptor with the following ``MPSGraph`` methods:
    /// - ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:initState:initCell:descriptor:name:``
    /// - ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
    /// - ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``
    /// - ``MPSGraph/LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:descriptor:name:``
    /// - ``MPSGraph/LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:inputWeight:bias:initState:initCell:descriptor:name:``
    /// - ``MPSGraph/LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:inputWeight:bias:initState:initCell:mask:descriptor:name:``
    /// - ``MPSGraph/LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:stateGradient:cellGradient:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphlstmdescriptor?language=objc)
    #[unsafe(super(MPSGraphObject, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "MPSGraphCore")]
    pub struct MPSGraphLSTMDescriptor;
);

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSCopying for MPSGraphLSTMDescriptor {}

#[cfg(feature = "MPSGraphCore")]
unsafe impl CopyingHelper for MPSGraphLSTMDescriptor {
    type Result = Self;
}

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSObjectProtocol for MPSGraphLSTMDescriptor {}

extern_methods!(
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphLSTMDescriptor {
        /// A parameter that defines time direction of the input sequence.
        ///
        /// If set to `YES` then the input sequence is passed in reverse time order to the layer.
        /// Note: Ignored when `bidirectional = YES`.
        /// Default value: `NO`.
        #[method(reverse)]
        pub unsafe fn reverse(&self) -> bool;

        /// Setter for [`reverse`][Self::reverse].
        #[method(setReverse:)]
        pub unsafe fn setReverse(&self, reverse: bool);

        /// A parameter that defines a bidirectional LSTM layer.
        ///
        /// If set to `YES` then the input sequence is traversed in both directions and the two results
        /// are concatenated together on the channel-axis.
        /// Default value: `NO`.
        #[method(bidirectional)]
        pub unsafe fn bidirectional(&self) -> bool;

        /// Setter for [`bidirectional`][Self::bidirectional].
        #[method(setBidirectional:)]
        pub unsafe fn setBidirectional(&self, bidirectional: bool);

        /// A parameter that controls whether or not to return the output cell from the LSTM layer.
        ///
        /// If set to `YES` then this layer will produce the internal cell of the LSTM unit as secondary output.
        /// Default value: `NO`.
        #[method(produceCell)]
        pub unsafe fn produceCell(&self) -> bool;

        /// Setter for [`produceCell`][Self::produceCell].
        #[method(setProduceCell:)]
        pub unsafe fn setProduceCell(&self, produce_cell: bool);

        /// A parameter that enables the LSTM layer to support training.
        ///
        /// If set to `YES` then the layer will produce training state tensor as a secondary output.
        /// Default value: `NO`.
        #[method(training)]
        pub unsafe fn training(&self) -> bool;

        /// Setter for [`training`][Self::training].
        #[method(setTraining:)]
        pub unsafe fn setTraining(&self, training: bool);

        /// A parameter that controls the internal order of the LSTM gates.
        ///
        /// If set to `YES` then the layer will use the gate-ordering `[ i, z, f, o ]` instead of default `[ i, f, z, o ]`.
        /// Default value: `NO`
        #[method(forgetGateLast)]
        pub unsafe fn forgetGateLast(&self) -> bool;

        /// Setter for [`forgetGateLast`][Self::forgetGateLast].
        #[method(setForgetGateLast:)]
        pub unsafe fn setForgetGateLast(&self, forget_gate_last: bool);

        /// A parameter that defines the activation function used with the input gate of the LSTM operation.
        ///
        /// Default value: `MPSGraphRNNActivationSigmoid`.
        #[method(inputGateActivation)]
        pub unsafe fn inputGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`inputGateActivation`][Self::inputGateActivation].
        #[method(setInputGateActivation:)]
        pub unsafe fn setInputGateActivation(&self, input_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function used with the forget gate of the LSTM operation.
        ///
        /// Default value: `MPSGraphRNNActivationSigmoid`.
        #[method(forgetGateActivation)]
        pub unsafe fn forgetGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`forgetGateActivation`][Self::forgetGateActivation].
        #[method(setForgetGateActivation:)]
        pub unsafe fn setForgetGateActivation(&self, forget_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function used with the cell gate of the LSTM operation.
        ///
        /// Default value: `MPSGraphRNNActivationTanh`.
        #[method(cellGateActivation)]
        pub unsafe fn cellGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`cellGateActivation`][Self::cellGateActivation].
        #[method(setCellGateActivation:)]
        pub unsafe fn setCellGateActivation(&self, cell_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function used with the output gate of the LSTM operation.
        ///
        /// Default value: `MPSGraphRNNActivationSigmoid`.
        #[method(outputGateActivation)]
        pub unsafe fn outputGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`outputGateActivation`][Self::outputGateActivation].
        #[method(setOutputGateActivation:)]
        pub unsafe fn setOutputGateActivation(&self, output_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function used with the current cell value of the LSTM operation.
        ///
        /// Default value: `MPSGraphRNNActivationTanh`.
        #[method(activation)]
        pub unsafe fn activation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`activation`][Self::activation].
        #[method(setActivation:)]
        pub unsafe fn setActivation(&self, activation: MPSGraphRNNActivation);

        /// Creates an LSTM descriptor with default values.
        #[method_id(@__retain_semantics Other descriptor)]
        pub unsafe fn descriptor() -> Option<Retained<Self>>;
    }
);

extern_methods!(
    /// Methods declared on superclass `NSObject`
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphLSTMDescriptor {
        #[method_id(@__retain_semantics Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__retain_semantics New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

extern_class!(
    /// The class that defines the parameters for a gated recurrent unit (GRU) operation.
    ///
    /// Use this descriptor with the following ``MPSGraph`` methods:
    /// - ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:descriptor:name:``
    /// - ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
    /// - ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:``
    /// - ``MPSGraph/GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:inputWeight:bias:descriptor:name:``
    /// - ``MPSGraph/GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:inputWeight:bias:initState:descriptor:name:``
    /// - ``MPSGraph/GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:stateGradient:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:``
    ///
    /// See also [Apple's documentation](https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphgrudescriptor?language=objc)
    #[unsafe(super(MPSGraphObject, NSObject))]
    #[derive(Debug, PartialEq, Eq, Hash)]
    #[cfg(feature = "MPSGraphCore")]
    pub struct MPSGraphGRUDescriptor;
);

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSCopying for MPSGraphGRUDescriptor {}

#[cfg(feature = "MPSGraphCore")]
unsafe impl CopyingHelper for MPSGraphGRUDescriptor {
    type Result = Self;
}

#[cfg(feature = "MPSGraphCore")]
unsafe impl NSObjectProtocol for MPSGraphGRUDescriptor {}

extern_methods!(
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphGRUDescriptor {
        /// A parameter that defines the time direction of the input sequence.
        ///
        /// If set to `YES` then the input sequence is passed in reverse time order to the layer.
        /// Note: Ignored when `bidirectional = YES`.
        /// Default value: `NO`.
        #[method(reverse)]
        pub unsafe fn reverse(&self) -> bool;

        /// Setter for [`reverse`][Self::reverse].
        #[method(setReverse:)]
        pub unsafe fn setReverse(&self, reverse: bool);

        /// A parameter that defines a bidirectional GRU layer.
        ///
        /// If set to `YES` then the input sequence is traversed in both directions and the two results
        /// are concatenated together on the channel-axis.
        /// Default value: `NO`.
        #[method(bidirectional)]
        pub unsafe fn bidirectional(&self) -> bool;

        /// Setter for [`bidirectional`][Self::bidirectional].
        #[method(setBidirectional:)]
        pub unsafe fn setBidirectional(&self, bidirectional: bool);

        /// A parameter that enables the GRU layer to support training.
        ///
        /// If set to `YES` then the layer will produce training state tensor as a secondary output.
        /// Default value: `NO`.
        #[method(training)]
        pub unsafe fn training(&self) -> bool;

        /// Setter for [`training`][Self::training].
        #[method(setTraining:)]
        pub unsafe fn setTraining(&self, training: bool);

        /// A parameter that controls the internal order of the GRU gates.
        ///
        /// If set to `YES` then the layer will use the gate-ordering `[ r, z, o ]` instead of default `[ z, r, o ]`.
        /// Default value: `NO`.
        #[method(resetGateFirst)]
        pub unsafe fn resetGateFirst(&self) -> bool;

        /// Setter for [`resetGateFirst`][Self::resetGateFirst].
        #[method(setResetGateFirst:)]
        pub unsafe fn setResetGateFirst(&self, reset_gate_first: bool);

        /// A parameter that chooses between two variants for the reset gate computation.
        ///
        /// If set to `YES` then the layer will compute the intermediate value as `c[t] = ( b + (h[t-1] m ) R^T) r[t]`.
        /// Otherwise it's computed as `c[t] = (h[t-1] r[t] m) R^T`.
        /// Default value: `NO`.
        #[method(resetAfter)]
        pub unsafe fn resetAfter(&self) -> bool;

        /// Setter for [`resetAfter`][Self::resetAfter].
        #[method(setResetAfter:)]
        pub unsafe fn setResetAfter(&self, reset_after: bool);

        /// A parameter that chooses between two variants for the final output computation.
        ///
        /// If set to `YES` then the layer will compute the final value as `h[t] = z[t] h[t-1] + (1-z[t]) o[t]`.
        /// Otherwise it's computed as `h[t] = (1-z[t]) h[t-1] + z[t] o[t]`.
        /// Default value: `NO`.
        #[method(flipZ)]
        pub unsafe fn flipZ(&self) -> bool;

        /// Setter for [`flipZ`][Self::flipZ].
        #[method(setFlipZ:)]
        pub unsafe fn setFlipZ(&self, flip_z: bool);

        /// A parameter that defines the activation function to use with the update-gate of the GRU operation.
        ///
        /// Default value: `MPSGraphRNNActivationSigmoid`.
        #[method(updateGateActivation)]
        pub unsafe fn updateGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`updateGateActivation`][Self::updateGateActivation].
        #[method(setUpdateGateActivation:)]
        pub unsafe fn setUpdateGateActivation(&self, update_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function to use with the reset-gate of the GRU operation.
        ///
        /// Default value: `MPSGraphRNNActivationSigmoid`.
        #[method(resetGateActivation)]
        pub unsafe fn resetGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`resetGateActivation`][Self::resetGateActivation].
        #[method(setResetGateActivation:)]
        pub unsafe fn setResetGateActivation(&self, reset_gate_activation: MPSGraphRNNActivation);

        /// A parameter that defines the activation function to use with the output-gate of the GRU operation.
        ///
        /// Default value: `MPSGraphRNNActivationTanh`.
        #[method(outputGateActivation)]
        pub unsafe fn outputGateActivation(&self) -> MPSGraphRNNActivation;

        /// Setter for [`outputGateActivation`][Self::outputGateActivation].
        #[method(setOutputGateActivation:)]
        pub unsafe fn setOutputGateActivation(&self, output_gate_activation: MPSGraphRNNActivation);

        /// Creates an GRU descriptor with default values.
        #[method_id(@__retain_semantics Other descriptor)]
        pub unsafe fn descriptor() -> Option<Retained<Self>>;
    }
);

extern_methods!(
    /// Methods declared on superclass `NSObject`
    #[cfg(feature = "MPSGraphCore")]
    unsafe impl MPSGraphGRUDescriptor {
        #[method_id(@__retain_semantics Init init)]
        pub unsafe fn init(this: Allocated<Self>) -> Retained<Self>;

        #[method_id(@__retain_semantics New new)]
        pub unsafe fn new() -> Retained<Self>;
    }
);

extern_methods!(
    /// MPSGraphRNNOps
    #[cfg(all(feature = "MPSGraph", feature = "MPSGraphCore"))]
    unsafe impl MPSGraph {
        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = x[t] W^T + (h[t-1]m) R^T + b
        /// h[t] = activation( z[t] ), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// See ``MPSGraphSingleGateRNNDescriptor`` for different `activation` options.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [2H,I] and otherwise it is [H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [2H] and otherwise it is [H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. This is useful for dropout support.
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid MPSGraphTensor array of size 1 or 2, depending on value of `descriptor.training`. The layout of the both outputs is [T,N,H] or [T,N,2H] for bidirectional.
        #[method_id(@__retain_semantics Other singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:)]
        pub unsafe fn singleGateRNNWithSourceTensor_recurrentWeight_inputWeight_bias_initState_mask_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = x[t] W^T + (h[t-1]m) R^T + b
        /// h[t] = activation( z[t] ), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// See ``MPSGraphSingleGateRNNDescriptor`` for different `activation` options.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [2H,I] and otherwise it is [H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [2H] and otherwise it is [H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid MPSGraphTensor array of size 1 or 2, depending on value of `descriptor.training`. The layout of the both outputs is [T,N,H] or [T,N,2H] for bidirectional.
        #[method_id(@__retain_semantics Other singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:)]
        pub unsafe fn singleGateRNNWithSourceTensor_recurrentWeight_inputWeight_bias_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = x[t] W^T + (h[t-1]m) R^T + b
        /// h[t] = activation( z[t] ), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// See ``MPSGraphSingleGateRNNDescriptor`` for different `activation` options.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid MPSGraphTensor array of size 1 or 2, depending on value of `descriptor.training`. The layout of the both outputs is [T,N,H] or [T,N,2H] for bidirectional.
        #[method_id(@__retain_semantics Other singleGateRNNWithSourceTensor:recurrentWeight:initState:descriptor:name:)]
        pub unsafe fn singleGateRNNWithSourceTensor_recurrentWeight_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// Note: For `bidirectional` this tensor must have a static shape.
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - stateGradient: The input gradient coming from the future timestep - optional, if missing the operation assumes zeroes.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [2H,I] and otherwise it is [H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [2H] and otherwise it is [H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. This is useful for dropout support.
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is `nil`, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias` and finally for `initState`.
        #[method_id(@__retain_semantics Other singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:stateGradient:inputWeight:bias:initState:mask:descriptor:name:)]
        pub unsafe fn singleGateRNNGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_stateGradient_inputWeight_bias_initState_mask_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            state_gradient: Option<&MPSGraphTensor>,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// Note: For `bidirectional` this tensor must have a static shape.
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [2H,I] and otherwise it is [H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [2H] and otherwise it is [H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. This is useful for dropout support.
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is `nil`, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias` and finally for `initState`.
        #[method_id(@__retain_semantics Other singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:inputWeight:bias:initState:mask:descriptor:name:)]
        pub unsafe fn singleGateRNNGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_inputWeight_bias_initState_mask_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// Note: For `bidirectional` this tensor must have a static shape.
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [2H,I] and otherwise it is [H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [2H] and otherwise it is [H].
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is `nil`, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias` and finally for `initState`.
        #[method_id(@__retain_semantics Other singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:inputWeight:bias:initState:descriptor:name:)]
        pub unsafe fn singleGateRNNGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_inputWeight_bias_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a single-gate RNN gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor that contains the source data `x[t]` with the data layout [T,N,I].
        /// In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,H] and
        /// for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,2H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,H,H] and otherwise it is [H,H].
        /// Note: For `bidirectional` this tensor must have a static shape.
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/singleGateRNNWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - initState: The initial internal state of the RNN `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the RNN operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is `nil`, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias` and finally for `initState`.
        #[method_id(@__retain_semantics Other singleGateRNNGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:initState:descriptor:name:)]
        pub unsafe fn singleGateRNNGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphSingleGateRNNDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM operation and returns the value tensor and optionally the cell state tensor and  the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `c` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = [i, f, z, o][t] = f( (h[t-1] m) R^T + x'[t] + p c[t-1] )
        /// x'[t] = x[t] W^T + b
        /// c[t] = f[t]c[t-1] + i[t]z[t]
        /// h[t] = o[t]g(c[t]), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `c[t]` is the second output (optional),
        /// `z[t]` is either the second or third output (optional), `h[-1]` is `initCell`.  and `h[-1]` is `initState`.
        /// `p` is an optional peephole vector.
        /// See ``MPSGraphLSTMDescriptor`` for different `activation` options for `f()` and `g()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix. For `bidirectional` the layout is [8H,I] and otherwise it is [4H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [8H] and otherwise it is [4H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. Useful for dropout.
        /// - peephole: A tensor containing the peephole vector `v` - optional, if missing the operation assumes zeroes. Shape is [4H], ie. a vector for each gate, or [2,4H] for bidirectional.
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 or 3, depending on values of `descriptor.produceCell` and `descriptor.training`.
        /// The layout of the both state and cell outputs are [T,N,H] or [T,N,2H] for bidirectional, and the layout of the trainingState output is [T,N,4H] or [T,N,8H] for bidirectional.
        #[method_id(@__retain_semantics Other LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:)]
        pub unsafe fn LSTMWithSourceTensor_recurrentWeight_inputWeight_bias_initState_initCell_mask_peephole_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            peephole: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM operation and returns the value tensor and optionally the cell state tensor and  the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `c` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = [i, f, z, o][t] = f( (h[t-1] m) R^T + x'[t] + p c[t-1] )
        /// x'[t] = x[t] W^T + b
        /// c[t] = f[t]c[t-1] + i[t]z[t]
        /// h[t] = o[t]g(c[t]), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `c[t]` is the second output (optional),
        /// `z[t]` is either the second or third output (optional), `h[-1]` is `initCell`.  and `h[-1]` is `initState`.
        /// `p` is an optional peephole vector.
        /// See ``MPSGraphLSTMDescriptor`` for different `activation` options for `f()` and `g()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix. For `bidirectional` the layout is [8H,I] and otherwise it is [4H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [8H] and otherwise it is [4H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 or 3, depending on values of `descriptor.produceCell` and `descriptor.training`.
        /// The layout of the both state and cell outputs are [T,N,H] or [T,N,2H] for bidirectional, and the layout of the trainingState output is [T,N,4H] or [T,N,8H] for bidirectional.
        #[method_id(@__retain_semantics Other LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:)]
        pub unsafe fn LSTMWithSourceTensor_recurrentWeight_inputWeight_bias_initState_initCell_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM operation and returns the value tensor and optionally the cell state tensor and  the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `c` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = [i, f, z, o][t] = f( (h[t-1] m) R^T + x'[t] + p c[t-1] )
        /// x'[t] = x[t] W^T + b
        /// c[t] = f[t]c[t-1] + i[t]z[t]
        /// h[t] = o[t]g(c[t]), where
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `c[t]` is the second output (optional),
        /// `z[t]` is either the second or third output (optional), `h[-1]` is `initCell`.  and `h[-1]` is `initState`.
        /// `p` is an optional peephole vector.
        /// See ``MPSGraphLSTMDescriptor`` for different `activation` options for `f()` and `g()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 or 3, depending on values of `descriptor.produceCell` and `descriptor.training`.
        /// The layout of the both state and cell outputs are [T,N,H] or [T,N,2H] for bidirectional, and the layout of the trainingState output is [T,N,4H] or [T,N,8H] for bidirectional.
        #[method_id(@__retain_semantics Other LSTMWithSourceTensor:recurrentWeight:initState:initCell:descriptor:name:)]
        pub unsafe fn LSTMWithSourceTensor_recurrentWeight_initState_initCell_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The third output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - cellOutputFwd: The second output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES` or `descriptor.produceCell = YES`.
        /// - stateGradient: The input gradient for state coming from the future timestep - optional, if missing the operation assumes zeroes.
        /// - cellGradient: Input gradient for cell coming from the future timestep - optional, if missing the operation assumes zeroes.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix. For `bidirectional` the layout is [8H,I] and otherwise it is [4H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [8H] and otherwise it is [4H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. Useful for dropout.
        /// - peephole: A tensor containing the peephole vector `v` - optional, if missing the operation assumes zeroes. Shape is [4H], ie. a vector for each gate, or [2,4H] for bidirectional.
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias`, for `peephole`, for `initState` and for `initCell`.
        #[method_id(@__retain_semantics Other LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:stateGradient:cellGradient:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:)]
        pub unsafe fn LSTMGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_cellOutputFwd_stateGradient_cellGradient_inputWeight_bias_initState_initCell_mask_peephole_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            cell_output_fwd: &MPSGraphTensor,
            state_gradient: Option<&MPSGraphTensor>,
            cell_gradient: Option<&MPSGraphTensor>,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            peephole: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The third output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - cellOutputFwd: The second output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES` or `descriptor.produceCell = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix. For `bidirectional` the layout is [8H,I] and otherwise it is [4H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [8H] and otherwise it is [4H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. Useful for dropout.
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias`, for `peephole`, for `initState` and for `initCell`.
        #[method_id(@__retain_semantics Other LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:inputWeight:bias:initState:initCell:mask:descriptor:name:)]
        pub unsafe fn LSTMGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_cellOutputFwd_inputWeight_bias_initState_initCell_mask_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            cell_output_fwd: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The third output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - cellOutputFwd: The second output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES` or `descriptor.produceCell = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix. For `bidirectional` the layout is [8H,I] and otherwise it is [4H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [8H] and otherwise it is [4H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - initCell: The initial internal cell of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes.
        /// For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias`, for `initState` and for `initCell`.
        #[method_id(@__retain_semantics Other LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:inputWeight:bias:initState:initCell:descriptor:name:)]
        pub unsafe fn LSTMGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_cellOutputFwd_inputWeight_bias_initState_initCell_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            cell_output_fwd: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            init_cell: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates an LSTM gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:mask:peephole:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]`  with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,4H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,8H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,4H,H] and otherwise it is [4H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The third output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - cellOutputFwd: The second output of
        /// ``MPSGraph/LSTMWithSourceTensor:recurrentWeight:inputWeight:bias:initState:initCell:descriptor:name:``
        /// with `descriptor.training = YES` or `descriptor.produceCell = YES`.
        /// - descriptor: A descriptor that defines the parameters for the LSTM operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias`, for `initState` and for `initCell`.
        #[method_id(@__retain_semantics Other LSTMGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:cellOutputFwd:descriptor:name:)]
        pub unsafe fn LSTMGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_cellOutputFwd_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            cell_output_fwd: &MPSGraphTensor,
            descriptor: &MPSGraphLSTMDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = fz( (h[t-1] m) R^T + x[t] W^T + b ),
        /// r[t] = fr( (h[t-1] m) R^T + x[t] W^T + b ),
        /// c[t] = (h[t-1] r[t] m) R^T
        /// o[t] = fo( c[t] + x[t] W^T + b )
        /// h[t] = z[t]h[t-1] + (1-z[t])o[t]
        /// ```
        /// If `resetAfter = YES` then `c[t]` is replaced by
        /// ```md
        /// c[t] = ( (h[t-1] m) R^T + b2 ) r[t]
        /// ```
        /// If `flipZ = YES` then `h[t]` is replaced by
        /// ```md
        /// h[t] = (1-z[t])h[t-1] + z[t]o[t].
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional  `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// `b2` is an optional `resetBias` vector, only used when `resetAfter = YES`.
        /// See ``MPSGraphGRUDescriptor`` for different `activation` options for `f()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. Useful for dropout.
        /// - secondaryBias: A tensor containing the secondary bias vector `b2` - optional, if missing the operation assumes zeroes. Only used with `reset_after = YES`. Shape is [H], ie. a vector for each gate, or [2H] for bidirectional.
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 depending on value of  `descriptor.training`.
        /// The layout of the state output is [T,N,H] or [T,N,2H] for bidirectional,
        /// and the layout of the `trainingState` output is [T,N,3H] or [T,N,6H] for bidirectional.
        #[method_id(@__retain_semantics Other GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:)]
        pub unsafe fn GRUWithSourceTensor_recurrentWeight_inputWeight_bias_initState_mask_secondaryBias_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            secondary_bias: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = fz( (h[t-1] m) R^T + x[t] W^T + b ),
        /// r[t] = fr( (h[t-1] m) R^T + x[t] W^T + b ),
        /// c[t] = (h[t-1] r[t] m) R^T
        /// o[t] = fo( c[t] + x[t] W^T + b )
        /// h[t] = z[t]h[t-1] + (1-z[t])o[t]
        /// ```
        /// If `resetAfter = YES` then `c[t]` is replaced by
        /// ```md
        /// c[t] = ( (h[t-1] m) R^T + b2 ) r[t]
        /// ```
        /// If `flipZ = YES` then `h[t]` is replaced by
        /// ```md
        /// h[t] = (1-z[t])h[t-1] + z[t]o[t].
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional  `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// `b2` is an optional `resetBias` vector, only used when `resetAfter = YES`.
        /// See ``MPSGraphGRUDescriptor`` for different `activation` options for `f()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 depending on value of  `descriptor.training`.
        /// The layout of the state output is [T,N,H] or [T,N,2H] for bidirectional,
        /// and the layout of the `trainingState` output is [T,N,3H] or [T,N,6H] for bidirectional.
        #[method_id(@__retain_semantics Other GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:)]
        pub unsafe fn GRUWithSourceTensor_recurrentWeight_inputWeight_bias_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU operation and returns the value and optionally the training state tensor.
        ///
        /// This operation returns tensors `h` and optionally `z` that are defined recursively as follows:
        /// ```md
        /// for t = 0 to T-1
        /// z[t] = fz( (h[t-1] m) R^T + x[t] W^T + b ),
        /// r[t] = fr( (h[t-1] m) R^T + x[t] W^T + b ),
        /// c[t] = (h[t-1] r[t] m) R^T
        /// o[t] = fo( c[t] + x[t] W^T + b )
        /// h[t] = z[t]h[t-1] + (1-z[t])o[t]
        /// ```
        /// If `resetAfter = YES` then `c[t]` is replaced by
        /// ```md
        /// c[t] = ( (h[t-1] m) R^T + b2 ) r[t]
        /// ```
        /// If `flipZ = YES` then `h[t]` is replaced by
        /// ```md
        /// h[t] = (1-z[t])h[t-1] + z[t]o[t].
        /// ```
        /// `W` is optional `inputWeight`, `R` is `recurrentWeight`, `b` is optional  `bias`, `m` is optional `mask`,
        /// `x[t]` is `source` `h[t]` is the first output, `z[t]` is the second output (optional) and `h[-1]` is `initState`.
        /// `b2` is an optional `resetBias` vector, only used when `resetAfter = YES`.
        /// See ``MPSGraphGRUDescriptor`` for different `activation` options for `f()`.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array of size 1 or 2 depending on value of  `descriptor.training`.
        /// The layout of the state output is [T,N,H] or [T,N,2H] for bidirectional,
        /// and the layout of the `trainingState` output is [T,N,3H] or [T,N,6H] for bidirectional.
        #[method_id(@__retain_semantics Other GRUWithSourceTensor:recurrentWeight:inputWeight:bias:descriptor:name:)]
        pub unsafe fn GRUWithSourceTensor_recurrentWeight_inputWeight_bias_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with  `descriptor.training = YES`.
        /// - outputFwd: The first output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - stateGradient: The input gradient for state coming from the future timestep - optional, if missing the operation assumes zeroes.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - mask: A tensor containing the mask `m` - optional, if missing the operation assumes ones. Useful for dropout.
        /// - secondaryBias: A tensor containing the secondary bias vector `b2` - optional, if missing the operation assumes zeroes. Only used with `reset_after = YES`. Shape is [H], ie. a vector for each gate, or [2H] for bidirectional.
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias`, for `initState` and for `secondaryBias`.
        #[method_id(@__retain_semantics Other GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:stateGradient:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:)]
        pub unsafe fn GRUGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_outputFwd_stateGradient_inputWeight_bias_initState_mask_secondaryBias_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            output_fwd: &MPSGraphTensor,
            state_gradient: Option<&MPSGraphTensor>,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            mask: Option<&MPSGraphTensor>,
            secondary_bias: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with  `descriptor.training = YES`.
        /// - outputFwd: The first output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - initState: The initial internal state of the LSTM `h[-1]` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [N,2H] and otherwise it is [N,H].
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight`, for `bias` and for `initState`.
        #[method_id(@__retain_semantics Other GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:inputWeight:bias:initState:descriptor:name:)]
        pub unsafe fn GRUGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_outputFwd_inputWeight_bias_initState_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            output_fwd: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            init_state: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates a GRU gradient operation and returns the gradient tensor values.
        ///
        /// For details of this operation and parameters, refer to documentation of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:mask:secondaryBias:descriptor:name:``.
        ///
        /// - Parameters:
        /// - source: A tensor containing the source data `x[t]` with the data layout [T,N,I]. In case `inputWeight = nil` and `bidirectional = NO` then the layout is [T,N,3H] and for `inputWeight = nil` and `bidirectional = YES` the layout is [T,N,6H].
        /// - recurrentWeight: A tensor containing the recurrent weights `R`. For `bidirectional` the layout is [2,3H,H] and otherwise it is [3H,H].
        /// - sourceGradient: The input gradient, that is the gradient of a tensor with respect to the first output of the forward pass.
        /// - zState: The second output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with  `descriptor.training = YES`.
        /// - outputFwd: The first output of
        /// ``MPSGraph/GRUWithSourceTensor:recurrentWeight:inputWeight:bias:initState:descriptor:name:``
        /// with `descriptor.training = YES`.
        /// - inputWeight: A tensor containing the input weights matrix `W` - optional, if missing the operation assumes a diagonal unit-matrix.
        /// For `bidirectional` the layout is [6H,I] and otherwise it is [3H,I].
        /// - bias: A tensor containing the bias `b` - optional, if missing the operation assumes zeroes. For `bidirectional` the layout is [6H] and otherwise it is [3H].
        /// - descriptor: A descriptor that defines the parameters for the GRU operation.
        /// - name: The name for the operation.
        /// - Returns: A valid `MPSGraphTensor` array containing gradients for each input tensor, except for `sourceGradient` and `mask`.
        /// In case an input is nil, no gradient will be returned for it.
        /// The order of the gradients will be: for `source`, for `recurrentWeight`, for `inputWeight` and for `bias`.
        #[method_id(@__retain_semantics Other GRUGradientsWithSourceTensor:recurrentWeight:sourceGradient:zState:outputFwd:inputWeight:bias:descriptor:name:)]
        pub unsafe fn GRUGradientsWithSourceTensor_recurrentWeight_sourceGradient_zState_outputFwd_inputWeight_bias_descriptor_name(
            &self,
            source: &MPSGraphTensor,
            recurrent_weight: &MPSGraphTensor,
            source_gradient: &MPSGraphTensor,
            z_state: &MPSGraphTensor,
            output_fwd: &MPSGraphTensor,
            input_weight: Option<&MPSGraphTensor>,
            bias: Option<&MPSGraphTensor>,
            descriptor: &MPSGraphGRUDescriptor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;
    }
);
