//! This file has been automatically generated by `objc2`'s `header-translator`.
//! DO NOT EDIT
use core::ptr::NonNull;
use objc2::__framework_prelude::*;
use objc2_foundation::*;

use crate::*;

extern_methods!(
    /// MPSGraphOptimizerOps
    #[cfg(all(feature = "MPSGraph", feature = "MPSGraphCore"))]
    unsafe impl MPSGraph {
        #[cfg(feature = "MPSGraphTensor")]
        /// The Stochastic gradient descent performs a gradient descent.
        ///
        /// `variable = variable - (learningRate * g)`
        /// where,
        /// `g` is gradient of error wrt variable
        ///
        /// - Parameters:
        /// - learningRateTensor: scalar tensor which indicates the learning rate to use with the optimizer
        /// - valuesTensor: values tensor, usually representing the trainable parameters
        /// - gradientTensor: partial gradient of the trainable parameters with respect to loss
        /// - name: name for the operation
        /// - Returns: A valid MPSGraphTensor object.
        #[unsafe(method_family(none))]
        #[method_id(stochasticGradientDescentWithLearningRateTensor:valuesTensor:gradientTensor:name:)]
        pub unsafe fn stochasticGradientDescentWithLearningRateTensor_valuesTensor_gradientTensor_name(
            &self,
            learning_rate_tensor: &MPSGraphTensor,
            values_tensor: &MPSGraphTensor,
            gradient_tensor: &MPSGraphTensor,
            name: Option<&NSString>,
        ) -> Retained<MPSGraphTensor>;

        #[cfg(all(
            feature = "MPSGraphMemoryOps",
            feature = "MPSGraphOperation",
            feature = "MPSGraphTensor"
        ))]
        /// The Stochastic gradient descent performs a gradient descent
        /// `variable = variable - (learningRate * g)`
        /// where,
        /// `g` is gradient of error wrt variable
        /// this op directly writes to the variable
        ///
        /// - Parameters:
        /// - learningRateTensor: scalar tensor which indicates the learning rate to use with the optimizer
        /// - variable: variable operation with trainable parameters
        /// - gradientTensor: partial gradient of the trainable parameters with respect to loss
        /// - name: name for the operation
        /// - Returns: A valid MPSGraphTensor object.
        #[unsafe(method_family(none))]
        #[method_id(applyStochasticGradientDescentWithLearningRateTensor:variable:gradientTensor:name:)]
        pub unsafe fn applyStochasticGradientDescentWithLearningRateTensor_variable_gradientTensor_name(
            &self,
            learning_rate_tensor: &MPSGraphTensor,
            variable: &MPSGraphVariableOp,
            gradient_tensor: &MPSGraphTensor,
            name: Option<&NSString>,
        ) -> Retained<MPSGraphOperation>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates operations to apply Adam optimization.
        ///
        /// The adam update ops are added
        /// current learning rate:
        /// ```md
        /// lr[t] = learningRate * sqrt(1 - beta2^t) / (1 - beta1^t)
        /// m[t] = beta1 * m[t-1] + (1 - beta1) * g
        /// v[t] = beta2 * v[t-1] + (1 - beta2) * (g ^ 2)
        /// maxVel[t] = max(maxVel[t-1], v[t])
        /// variable = variable - lr[t] * m[t] / (sqrt(maxVel) + epsilon)
        /// ```
        /// - Parameters:
        /// - learningRateTensor: scalar tensor which indicates the learning rate to use with the optimizer
        /// - beta1Tensor: beta1Tensor
        /// - beta2Tensor: beta2Tensor
        /// - beta1PowerTensor: `beta1^t` beta1 power tensor
        /// - beta2PowerTensor: `beta2^t` beta2 power tensor
        /// - valuesTensor: values to update with optimization
        /// - momentumTensor: momentum tensor
        /// - velocityTensor: velocity tensor
        /// - maximumVelocityTensor: optional maximum velocity tensor
        /// - gradientTensor: partial gradient of the trainable parameters with respect to loss
        /// - name: name for the operation
        /// - Returns: if maximumVelocity is nil array of 3 tensors (update, newMomentum, newVelocity) else array of 4 tensors (update, newMomentum, newVelocity, newMaximumVelocity)
        #[unsafe(method_family(none))]
        #[method_id(adamWithLearningRateTensor:beta1Tensor:beta2Tensor:epsilonTensor:beta1PowerTensor:beta2PowerTensor:valuesTensor:momentumTensor:velocityTensor:maximumVelocityTensor:gradientTensor:name:)]
        pub unsafe fn adamWithLearningRateTensor_beta1Tensor_beta2Tensor_epsilonTensor_beta1PowerTensor_beta2PowerTensor_valuesTensor_momentumTensor_velocityTensor_maximumVelocityTensor_gradientTensor_name(
            &self,
            learning_rate_tensor: &MPSGraphTensor,
            beta1_tensor: &MPSGraphTensor,
            beta2_tensor: &MPSGraphTensor,
            epsilon_tensor: &MPSGraphTensor,
            beta1_power_tensor: &MPSGraphTensor,
            beta2_power_tensor: &MPSGraphTensor,
            values_tensor: &MPSGraphTensor,
            momentum_tensor: &MPSGraphTensor,
            velocity_tensor: &MPSGraphTensor,
            maximum_velocity_tensor: Option<&MPSGraphTensor>,
            gradient_tensor: &MPSGraphTensor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;

        #[cfg(feature = "MPSGraphTensor")]
        /// Creates operations to apply Adam optimization.
        ///
        /// The adam update ops are added
        /// ```md
        /// m[t] = beta1m[t-1] + (1 - beta1) * g
        /// v[t] = beta2v[t-1] + (1 - beta2) * (g ^ 2)
        /// maxVel[t] = max(maxVel[t-1],v[t])
        /// variable = variable - lr[t] * m[t] / (sqrt(maxVel) + epsilon)
        /// ```
        /// - Parameters:
        /// - learningRateTensor: scalar tensor which indicates the learning rate to use with the optimizer
        /// - beta1Tensor: beta1Tensor
        /// - beta2Tensor: beta2Tensor
        /// - epsilonTensor: epsilon tensor
        /// - valuesTensor: values to update with optimization
        /// - momentumTensor: momentum tensor
        /// - velocityTensor: velocity tensor
        /// - maximumVelocityTensor: optional maximum velocity tensor
        /// - gradientTensor: partial gradient of the trainable parameters with respect to loss
        /// - name: name for the operation
        /// - Returns: if maximumVelocity is nil array of 3 tensors (update, newMomentum, newVelocity) else array of 4 tensors (update, newMomentum, newVelocity, newMaximumVelocity)
        #[unsafe(method_family(none))]
        #[method_id(adamWithCurrentLearningRateTensor:beta1Tensor:beta2Tensor:epsilonTensor:valuesTensor:momentumTensor:velocityTensor:maximumVelocityTensor:gradientTensor:name:)]
        pub unsafe fn adamWithCurrentLearningRateTensor_beta1Tensor_beta2Tensor_epsilonTensor_valuesTensor_momentumTensor_velocityTensor_maximumVelocityTensor_gradientTensor_name(
            &self,
            current_learning_rate_tensor: &MPSGraphTensor,
            beta1_tensor: &MPSGraphTensor,
            beta2_tensor: &MPSGraphTensor,
            epsilon_tensor: &MPSGraphTensor,
            values_tensor: &MPSGraphTensor,
            momentum_tensor: &MPSGraphTensor,
            velocity_tensor: &MPSGraphTensor,
            maximum_velocity_tensor: Option<&MPSGraphTensor>,
            gradient_tensor: &MPSGraphTensor,
            name: Option<&NSString>,
        ) -> Retained<NSArray<MPSGraphTensor>>;
    }
);
